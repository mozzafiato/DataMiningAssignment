{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliarymethods import datasets as dp\n",
    "from auxiliarymethods.reader import tud_to_networkx\n",
    "import auxiliarymethods.auxiliary_methods as aux\n",
    "import os\n",
    "import numpy as np\n",
    "from lib import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "iterations = 5 # weierfeiler-lehman iterations\n",
    "k = 120\n",
    "alpha = .85\n",
    "delta_affine = 1.5\n",
    "delta_dist = .5\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickling\n",
    "pickle_path = 'pickles'\n",
    "# files can be found here\n",
    "# https://ucloud.univie.ac.at/index.php/s/pjLEBg8rCJWdaJ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_csv(path):\n",
    "    return np.loadtxt(path, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\"kernels\", \"without_labels\")\n",
    "ds_name = \"IMDB-BINARY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_name = \"IMDB-BINARY\"\n",
    "classes = dp.get_dataset(ds_name)\n",
    "G = tud_to_networkx(ds_name)\n",
    "print(f\"Number of graphs in data set is {len(G)}\")\n",
    "print(f\"Number of classes {len(set(classes.tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gram Matrix for the Weisfeiler-Lehman subtree kernel\n",
    "gram = load_csv(os.path.join(base_path,f\"{ds_name}_gram_matrix_wl{iterations}.csv\"))\n",
    "gram = aux.normalize_gram_matrix(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning\n",
    "point_info, partitions = make_partitions(gram, k=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing\n",
    "pickle.dump(point_info, open(os.path.join(pickle_path, f'point_info_{iterations}_{k}_{alpha}.p'), 'wb'))\n",
    "pickle.dump(partitions, open(os.path.join(pickle_path, f'partitions_{iterations}_{k}_{alpha}.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading\n",
    "point_info = pickle.load(open(os.path.join(pickle_path, f'point_info_{iterations}_{k}_{alpha}.p'), 'rb'))\n",
    "partitions = pickle.load(open(os.path.join(pickle_path, f'partitions_{iterations}_{k}_{alpha}.p'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "models, clusters = cluster_partitions(gram, partitions, point_info, 1.5, .5, 2)\n",
    "cluster_info = compute_cluster_list(clusters, gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing\n",
    "pickle.dump(models, open(os.path.join(pickle_path, f'models_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "pickle.dump(clusters, open(os.path.join(pickle_path, f'clusters_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "pickle.dump(cluster_info, open(os.path.join(pickle_path, f'cluster_info_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading\n",
    "models = pickle.load(open(os.path.join(pickle_path, f'models_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "clusters = pickle.load(open(os.path.join(pickle_path, f'clusters_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "cluster_info = pickle.load(open(os.path.join(pickle_path, f'cluster_info_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy\n",
    "hierarchy = build_hierarchy(cluster_info, delta_affine=1.5, delta_dist=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing\n",
    "pickle.dump(hierarchy, open(os.path.join(pickle_path, f'hierarchy_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading\n",
    "hierarchy = pickle.load(open(os.path.join(pickle_path, f'hierarchy_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2cf64d48c5d50ea1e2870a475fdf6588c9274e8b6509a1571fe1129ef0ff186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
