{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/Documents/uni/WS22/DM/P1/DataMiningAssignment/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from auxiliarymethods import datasets as dp\n",
    "from auxiliarymethods.reader import tud_to_networkx\n",
    "import auxiliarymethods.auxiliary_methods as aux\n",
    "import os\n",
    "import numpy as np\n",
    "from lib import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "iterations = 5 # weierfeiler-lehman iterations\n",
    "k = 900\n",
    "alpha = .85\n",
    "delta_affine = .5\n",
    "delta_dist = .5\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling\n",
    "pickle_path = 'pickles'\n",
    "# files can be found here\n",
    "# https://ucloud.univie.ac.at/index.php/s/pjLEBg8rCJWdaJ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_csv(path):\n",
    "    return np.loadtxt(path, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\"kernels\", \"without_labels\")\n",
    "ds_name = \"IMDB-BINARY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gram Matrix for the Weisfeiler-Lehman subtree kernel\n",
    "try:\n",
    "    gram = load_csv(os.path.join(base_path,f\"{ds_name}_gram_matrix_wl{iterations}.csv\"))\n",
    "except:\n",
    "    ds_name = \"IMDB-BINARY\"\n",
    "    classes = dp.get_dataset(ds_name)\n",
    "    G = tud_to_networkx(ds_name)\n",
    "    print(f\"Number of graphs in data set is {len(G)}\")\n",
    "    print(f\"Number of classes {len(set(classes.tolist()))}\")\n",
    "    gram = load_csv(os.path.join(base_path,f\"{ds_name}_gram_matrix_wl{iterations}.csv\"))\n",
    "finally:\n",
    "    gram = aux.normalize_gram_matrix(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning\n",
    "try:\n",
    "    # reading\n",
    "    point_info = pickle.load(open(os.path.join(pickle_path, f'point_info_{iterations}_{k}_{alpha}.p'), 'rb'))\n",
    "    partitions = pickle.load(open(os.path.join(pickle_path, f'partitions_{iterations}_{k}_{alpha}.p'), 'rb'))\n",
    "except:\n",
    "    # *** COMPUTATION ***\n",
    "    point_info, partitions = make_partitions(gram, k, alpha)\n",
    "    # writing\n",
    "    pickle.dump(point_info, open(os.path.join(pickle_path, f'point_info_{iterations}_{k}_{alpha}.p'), 'wb'))\n",
    "    pickle.dump(partitions, open(os.path.join(pickle_path, f'partitions_{iterations}_{k}_{alpha}.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "try:\n",
    "    # reading\n",
    "    models = pickle.load(open(os.path.join(pickle_path, f'models_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "    clusters = pickle.load(open(os.path.join(pickle_path, f'clusters_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "    cluster_info = pickle.load(open(os.path.join(pickle_path, f'cluster_info_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "except:\n",
    "    # *** COMPUTATION ***\n",
    "    models, clusters = cluster_partitions(gram, partitions, point_info, delta_affine, delta_dist, min_samples)\n",
    "    cluster_info = compute_cluster_list(clusters, gram)\n",
    "    # writing\n",
    "    pickle.dump(models, open(os.path.join(pickle_path, f'models_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "    pickle.dump(clusters, open(os.path.join(pickle_path, f'clusters_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "    pickle.dump(cluster_info, open(os.path.join(pickle_path, f'cluster_info_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_hierarchy() missing 1 required positional argument: 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[39m# reading\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     hierarchy \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(pickle_path, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhierarchy_\u001b[39;49m\u001b[39m{\u001b[39;49;00miterations\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00malpha\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdelta_affine\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdelta_dist\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mmin_samples\u001b[39m}\u001b[39;49;00m\u001b[39m.p\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[39m# *** COMPUTATION ***\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/uni/WS22/DM/P1/DataMiningAssignment/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickles/hierarchy_5_900_0.85_0.5_0.5_2.p'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     hierarchy \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pickle_path, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhierarchy_\u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00malpha\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdelta_affine\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdelta_dist\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmin_samples\u001b[39m}\u001b[39;00m\u001b[39m.p\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[39m# *** COMPUTATION ***\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     hierarchy \u001b[39m=\u001b[39m build_hierarchy(cluster_info, delta_affine, delta_dist)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# writing\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     pickle\u001b[39m.\u001b[39mdump(hierarchy, \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pickle_path, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhierarchy_\u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00malpha\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdelta_affine\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdelta_dist\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmin_samples\u001b[39m}\u001b[39;00m\u001b[39m.p\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: build_hierarchy() missing 1 required positional argument: 'd'"
     ]
    }
   ],
   "source": [
    "# hierarchy\n",
    "try:\n",
    "    # reading\n",
    "    hierarchy = pickle.load(open(os.path.join(pickle_path, f'hierarchy_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "except:\n",
    "    # *** COMPUTATION ***\n",
    "    hierarchy = build_hierarchy(cluster_info, delta_affine, delta_dist, 1000)\n",
    "    # writing\n",
    "    pickle.dump(hierarchy, open(os.path.join(pickle_path, f'hierarchy_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elki_parser import draw_graph\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(13)\n",
    "fig.set_figwidth(17)\n",
    "draw_graph(hierarchy, fig.get_axes()[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2cf64d48c5d50ea1e2870a475fdf6588c9274e8b6509a1571fe1129ef0ff186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
