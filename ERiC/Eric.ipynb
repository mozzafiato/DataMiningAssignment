{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliarymethods import datasets as dp\n",
    "from auxiliarymethods.reader import tud_to_networkx\n",
    "import auxiliarymethods.auxiliary_methods as aux\n",
    "import os\n",
    "import numpy as np\n",
    "from lib import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "iterations = 5 # weierfeiler-lehman iterations\n",
    "k = 120\n",
    "alpha = .85\n",
    "delta_affine = 1.5\n",
    "delta_dist = .5\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling\n",
    "pickle_path = 'pickles'\n",
    "# files can be found here\n",
    "# https://ucloud.univie.ac.at/index.php/s/pjLEBg8rCJWdaJ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_csv(path):\n",
    "    return np.loadtxt(path, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\"kernels\", \"without_labels\")\n",
    "ds_name = \"IMDB-BINARY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gram Matrix for the Weisfeiler-Lehman subtree kernel\n",
    "try:\n",
    "    gram = load_csv(os.path.join(base_path,f\"{ds_name}_gram_matrix_wl{iterations}.csv\"))\n",
    "except:\n",
    "    ds_name = \"IMDB-BINARY\"\n",
    "    classes = dp.get_dataset(ds_name)\n",
    "    G = tud_to_networkx(ds_name)\n",
    "    print(f\"Number of graphs in data set is {len(G)}\")\n",
    "    print(f\"Number of classes {len(set(classes.tolist()))}\")\n",
    "    gram = load_csv(os.path.join(base_path,f\"{ds_name}_gram_matrix_wl{iterations}.csv\"))\n",
    "finally:\n",
    "    gram = aux.normalize_gram_matrix(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning\n",
    "try:\n",
    "    # reading\n",
    "    point_info = pickle.load(open(os.path.join(pickle_path, f'point_info_{iterations}_{k}_{alpha}.p'), 'rb'))\n",
    "    partitions = pickle.load(open(os.path.join(pickle_path, f'partitions_{iterations}_{k}_{alpha}.p'), 'rb'))\n",
    "except:\n",
    "    # *** COMPUTATION ***\n",
    "    point_info, partitions = make_partitions(gram, k)\n",
    "    # writing\n",
    "    pickle.dump(point_info, open(os.path.join(pickle_path, f'point_info_{iterations}_{k}_{alpha}.p'), 'wb'))\n",
    "    pickle.dump(partitions, open(os.path.join(pickle_path, f'partitions_{iterations}_{k}_{alpha}.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "try:\n",
    "    # reading\n",
    "    models = pickle.load(open(os.path.join(pickle_path, f'models_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "    clusters = pickle.load(open(os.path.join(pickle_path, f'clusters_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "    cluster_info = pickle.load(open(os.path.join(pickle_path, f'cluster_info_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "except:\n",
    "    # *** COMPUTATION ***\n",
    "    models, clusters = cluster_partitions(gram, partitions, point_info, delta_affine, delta_dist, min_samples)\n",
    "    cluster_info = compute_cluster_list(clusters, gram)\n",
    "    # writing\n",
    "    pickle.dump(models, open(os.path.join(pickle_path, f'models_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "    pickle.dump(clusters, open(os.path.join(pickle_path, f'clusters_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "    pickle.dump(cluster_info, open(os.path.join(pickle_path, f'cluster_info_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy\n",
    "try:\n",
    "    # reading\n",
    "    hierarchy = pickle.load(open(os.path.join(pickle_path, f'hierarchy_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'rb'))\n",
    "except:\n",
    "    # *** COMPUTATION ***\n",
    "    hierarchy = build_hierarchy(cluster_info, delta_affine, delta_dist)\n",
    "    # writing\n",
    "    pickle.dump(hierarchy, open(os.path.join(pickle_path, f'hierarchy_{iterations}_{k}_{alpha}_{delta_affine}_{delta_dist}_{min_samples}.p'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2cf64d48c5d50ea1e2870a475fdf6588c9274e8b6509a1571fe1129ef0ff186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
